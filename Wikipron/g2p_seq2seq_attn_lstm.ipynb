{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "g2p_seq2seq_attn_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdiSfWlC2BukBjhS/OPXWl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deterministic-algorithms-lab/Speech-Explorations/blob/master/Wikipron/g2p_seq2seq_attn_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFgndp8ozT11"
      },
      "source": [
        "# Installing and Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOCnw0vnehKT"
      },
      "source": [
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install git+git://github.com/deepmind/optax.git\n",
        "!pip install -q -U trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vZsBj6BekmL"
      },
      "source": [
        "import haiku as hk\n",
        "import jax.numpy as jnp\n",
        "import jax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw0XDtyFAyac"
      },
      "source": [
        "lg = 'eng' "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4aE1L7LzgKT"
      },
      "source": [
        "config = {\n",
        "    #data params\n",
        "    'lang' : lg,\n",
        "    'n_epochs' : 500,\n",
        "    'batch_size' : 4,\n",
        "    'data_files' : [f'{lg}.tsv'],\n",
        "    'input_vocab_size' : None,\n",
        "    'output_vocab_size' : None, \n",
        "    \n",
        "    #optimizer params\n",
        "    'max_grad_norm' : 1,\n",
        "    'learning_rate' : 1e-4,\n",
        "\n",
        "    #Model Params\n",
        "    'd_model' : 512,\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeHBvyD9x1eC"
      },
      "source": [
        "# Getting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDtjTU3dx4PD"
      },
      "source": [
        "!pip install wikipron"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhfrQ4hex_nT"
      },
      "source": [
        "#!wikipron {lg} > {lg}.tsv\n",
        "!shuf {lg}.tsv > shuf_{lg}.tsv\n",
        "!rm {lg}.tsv\n",
        "!mv shuf_{lg}.tsv {lg}.tsv"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80O74jAIz1l6"
      },
      "source": [
        "import pandas as pd\n",
        "import functools\n",
        "\n",
        "class tsv_loader:\n",
        "    \n",
        "    def __init__(self, config):\n",
        "       self.config = config\n",
        "       self.df = pd.read_csv(f'{lg}.tsv', sep='\\t', names=['graphemes', 'phonemes'])\n",
        "       self.setup_data()\n",
        "       self.unique = {}\n",
        "       self.unique['graphemes'] = self.get_unique(key='graphemes')\n",
        "       self.unique['phonemes'] = self.get_unique(key='phonemes')\n",
        "       self.token_to_id = {}\n",
        "       self.id_to_token = {}\n",
        "       self.generate_encodings(key='graphemes')\n",
        "       self.generate_encodings(key='phonemes')\n",
        "       self.set_config()\n",
        "       \n",
        "    def setup_data(self):\n",
        "        \n",
        "        def split_func(x):\n",
        "            x['graphemes'] = [char for char in x['graphemes'].strip()]\n",
        "            x['phonemes'] = x['phonemes'].strip().split()\n",
        "\n",
        "        self.df.apply(split_func, axis=1)\n",
        "\n",
        "    def get_unique(self, key):\n",
        "        return functools.reduce(lambda a,b: set(a).union(set(b)), self.df[key].to_list(), set([]))\n",
        "    \n",
        "    def generate_encodings(self, key):\n",
        "        self.token_to_id[key]= {}\n",
        "        self.token_to_id[key]['<pad>'] = len(self.token_to_id[key])\n",
        "        self.token_to_id[key]['<s>'] = len(self.token_to_id[key])\n",
        "        self.token_to_id[key]['</s>'] = len(self.token_to_id[key])\n",
        "\n",
        "        self.token_to_id[key].update( { token : id+3 for id, token in enumerate(list(self.unique[key])) } )\n",
        "        self.id_to_token[key] = { id : token for token, id in self.token_to_id[key].items() }\n",
        "\n",
        "    def set_config(self):\n",
        "        self.config['max_length'] = {}\n",
        "        self.config['max_length']['graphemes'] = functools.reduce(lambda a,b : max(a,len(b)), self.df['graphemes'], 0) + 2\n",
        "        self.config['max_length']['phonemes'] = functools.reduce(lambda a,b : max(a,len(b)), self.df['phonemes'], 0) + 2\n",
        "        self.config['input_vocab_size'] = len(self.token_to_id['graphemes'])\n",
        "        self.config['output_vocab_size'] = len(self.token_to_id['phonemes'])\n",
        "        \n",
        "    def generate_batches(self):\n",
        "        i = 0\n",
        "        while i<len(self.df)//self.config['batch_size']:\n",
        "            graphemes_batch = self.df['graphemes'][i:i+self.config['batch_size']]\n",
        "            phonemes_batch = self.df['phonemes'][i:i+self.config['batch_size']]\n",
        "            i+=1\n",
        "            yield graphemes_batch, phonemes_batch\n",
        "    \n",
        "    def encode_batch(self, batch, key='graphemes', add_eos=True):\n",
        "        padded_batch = []\n",
        "        \n",
        "        for elem in batch:\n",
        "            padded_batch.append( ['<s>'] + elem + (['</s>'] if add_eos else [])\n",
        "                                +['<pad>']\n",
        "                                *(self.config['max_length'][key]-len(elem)) \n",
        "                               )\n",
        "        return    jnp.asarray( [ [self.token_to_id[key][token] \n",
        "                                  for token in elem] \n",
        "                                  for elem in padded_batch], dtype=jnp.int16)\n",
        "        \n",
        "    def decode_batch(self, batch, key='phonemes'):\n",
        "        decoded_batch = []\n",
        "        for elem in batch :\n",
        "            decoded_seq = []\n",
        "            for id in elem:\n",
        "                if id==self.token_to_id[key]['</s>']:\n",
        "                    break\n",
        "                if id!=self.token_to_id[key]['<pad>'] and id!=self.token_to_id[key]['<s>']:\n",
        "                    decoded_seq.append(self.id_to_token[key][int(id)])\n",
        "            decoded_batch.append(decoded_seq)\n",
        "        return decoded_batch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZkZl8TxPv60"
      },
      "source": [
        "data_loader = tsv_loader(config)\n",
        "n_examples = len(data_loader.df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs9e14CpU6_K"
      },
      "source": [
        "# Trax LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzSHc0PzU8nO"
      },
      "source": [
        "from trax.models.rnn import LSTMSeq2SeqAttn\n",
        "from trax.layers.metrics import CategoryCrossEntropy\n",
        "from trax import shapes"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4PuPmzOU-OT"
      },
      "source": [
        "lstm = LSTMSeq2SeqAttn(input_vocab_size=config['input_vocab_size'],\n",
        "                       target_vocab_size=config['output_vocab_size'],\n",
        "                       d_model=config['d_model'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjCQqiRYZ_1u"
      },
      "source": [
        "grapheme_batch, phoneme_batch = next(data_loader.generate_batches())\n",
        "input = data_loader.encode_batch(grapheme_batch)\n",
        "labels = data_loader.encode_batch(phoneme_batch, key='phonemes')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goflBHSTafpM"
      },
      "source": [
        "lstm.init_weights_and_state((shapes.signature(input), shapes.signature(labels)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihbw2FfN6qyi"
      },
      "source": [
        "## Loss and Update Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr8ZvKnlmpwr"
      },
      "source": [
        "def loss_fn(params, input, labels, mask):\n",
        "    prev_weights = lstm.weights\n",
        "    lstm.weights = params\n",
        "    logits, labels = lstm((input, labels))\n",
        "    logits, labels = logits[:,:-1,:], labels[:, 1:]\n",
        "    logits = jax.vmap(jnp.multiply, in_axes=(2, None), out_axes=2)(logits, labels!=0)\n",
        "    loss = CategoryCrossEntropy()((logits, labels))\n",
        "    lstm.weights = prev_weights\n",
        "    return loss\n",
        "\n",
        "@jax.jit\n",
        "def update(params, opt_state, input, labels, mask):\n",
        "    batch_loss, grads = jax.value_and_grad(loss_fn)(params, input, labels, mask)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state, batch_loss\n",
        "\n",
        "params = lstm.weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eEy44-7zH_6"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek7ebkjDzHYd"
      },
      "source": [
        "import optax\n",
        "import numpy as np"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-01jwfmzM0v"
      },
      "source": [
        "def make_lr_schedule(warmup_percentage, total_steps):\n",
        "    \n",
        "    def lr_schedule(step):\n",
        "        percent_complete = step/total_steps\n",
        "        \n",
        "        #0 or 1 based on whether we are before peak\n",
        "        before_peak = jax.lax.convert_element_type((percent_complete<=warmup_percentage),\n",
        "                                                   np.float32)\n",
        "        #Factor for scaling learning rate\n",
        "        scale = ( before_peak*(percent_complete/warmup_percentage)\n",
        "                + (1-before_peak) ) * (1-percent_complete)\n",
        "        \n",
        "        return scale\n",
        "    \n",
        "    return lr_schedule"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ8XY_iPzQLv"
      },
      "source": [
        "total_steps = config['n_epochs']*(n_examples//config['batch_size'])\n",
        "\n",
        "lr_schedule = make_lr_schedule(warmup_percentage=0.1, total_steps=total_steps)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ5TpEvpzSmP"
      },
      "source": [
        "opt = optax.chain(\n",
        "    optax.clip_by_global_norm(config['max_grad_norm']),\n",
        "    optax.adam(learning_rate=config['learning_rate']),\n",
        "    optax.scale_by_schedule(lr_schedule),\n",
        ")\n",
        "opt_state = opt.init(params)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcswKI4KaEip"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP2zooQBaJ9A",
        "outputId": "6a6d6ac4-564b-4044-ac23-cbbccad772df"
      },
      "source": [
        "for _ in range(config['n_epochs']):\n",
        "    losses = []\n",
        "    i=0\n",
        "    for grapheme_batch, phoneme_batch in data_loader.generate_batches():\n",
        "        input = data_loader.encode_batch(grapheme_batch)\n",
        "        labels = data_loader.encode_batch(phoneme_batch, key='phonemes')\n",
        "        mask = list(labels!=0)\n",
        "        params, opt_state, batch_loss = update(params, opt_state, input, labels, mask)\n",
        "        losses.append(batch_loss)\n",
        "        if i%100==0:\n",
        "            print(sum(losses)/100)\n",
        "            losses = []\n",
        "        i+=1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.04482197\n",
            "0.044597864\n",
            "0.043900125\n",
            "0.042742714\n",
            "0.04131021\n",
            "0.040239327\n",
            "0.0397346\n",
            "0.039486874\n",
            "0.03934227\n",
            "0.039235782\n",
            "0.039152663\n",
            "0.03908205\n",
            "0.038992964\n",
            "0.03889775\n",
            "0.038761962\n",
            "0.038601343\n",
            "0.03842967\n",
            "0.03836578\n",
            "0.038281117\n",
            "0.038347516\n",
            "0.038225465\n",
            "0.038306188\n",
            "0.03782034\n",
            "0.037672333\n",
            "0.03761198\n",
            "0.037461594\n",
            "0.037201207\n",
            "0.037030663\n",
            "0.036825392\n",
            "0.0365864\n",
            "0.03643137\n",
            "0.036413636\n",
            "0.03621721\n",
            "0.036176622\n",
            "0.036547292\n",
            "0.036158137\n",
            "0.035956524\n",
            "0.036052078\n",
            "0.03617616\n",
            "0.03566277\n",
            "0.035829414\n",
            "0.03896674\n",
            "0.03678335\n",
            "0.039515298\n",
            "0.037940636\n",
            "0.03844556\n",
            "0.036840256\n",
            "0.03828436\n",
            "0.037817996\n",
            "0.036145147\n",
            "0.036238234\n",
            "0.03681084\n",
            "0.03743762\n",
            "0.037317835\n",
            "0.035944905\n",
            "0.035325892\n",
            "0.03505342\n",
            "0.035487898\n",
            "0.034950305\n",
            "0.03482866\n",
            "0.034202993\n",
            "0.03464408\n",
            "0.03421261\n",
            "0.033963185\n",
            "0.03424811\n",
            "0.034192257\n",
            "0.034341436\n",
            "0.03404871\n",
            "0.03528464\n",
            "0.03434643\n",
            "0.033301167\n",
            "0.03433478\n",
            "0.03333162\n",
            "0.03420208\n",
            "0.03343512\n",
            "0.03306187\n",
            "0.034235395\n",
            "0.033275098\n",
            "0.033736177\n",
            "0.032264438\n",
            "0.03257799\n",
            "0.0328663\n",
            "0.031970326\n",
            "0.034451813\n",
            "0.03183285\n",
            "0.03180996\n",
            "0.03193228\n",
            "0.033445805\n",
            "0.03192705\n",
            "0.03146201\n",
            "0.03125623\n",
            "0.031495694\n",
            "0.032154568\n",
            "0.03168939\n",
            "0.031207405\n",
            "0.031011207\n",
            "0.031227209\n",
            "0.031897943\n",
            "0.031192392\n",
            "0.031068593\n",
            "0.03397791\n",
            "0.03185768\n",
            "0.032419164\n",
            "0.030898651\n",
            "0.030851444\n",
            "0.031125879\n",
            "0.030600617\n",
            "0.03144957\n",
            "0.030698568\n",
            "0.030589677\n",
            "0.030670732\n",
            "0.03126781\n",
            "0.03036396\n",
            "0.030567067\n",
            "0.031017117\n",
            "0.030510044\n",
            "0.03102189\n",
            "0.030415894\n",
            "0.03037039\n",
            "0.03026663\n",
            "0.03081687\n",
            "0.030384643\n",
            "0.030279761\n",
            "0.030989323\n",
            "0.03126924\n",
            "0.030348497\n",
            "0.030289762\n",
            "0.030520527\n",
            "0.030284392\n",
            "0.03016818\n",
            "0.030284602\n",
            "0.030471941\n",
            "0.030141432\n",
            "0.03015624\n",
            "0.030971825\n",
            "0.030328056\n",
            "0.030256748\n",
            "0.030157294\n",
            "0.030129949\n",
            "0.030319685\n",
            "0.03010113\n",
            "0.03009184\n",
            "0.030113395\n",
            "0.030074162\n",
            "0.030162655\n",
            "0.030121833\n",
            "0.030030161\n",
            "0.030025432\n",
            "0.030056257\n",
            "0.030049568\n",
            "0.030086368\n",
            "0.030244846\n",
            "0.030035935\n",
            "0.030002318\n",
            "0.030010318\n",
            "0.02998393\n",
            "0.029992277\n",
            "0.02999281\n",
            "0.029992333\n",
            "0.029988056\n",
            "0.030002508\n",
            "0.029977864\n",
            "0.029997978\n",
            "0.029963486\n",
            "0.029971335\n",
            "0.029967112\n",
            "0.029962242\n",
            "0.029961808\n",
            "0.029959394\n",
            "0.029954132\n",
            "0.029956548\n",
            "0.02994995\n",
            "0.029952582\n",
            "0.029946217\n",
            "0.029948222\n",
            "0.029941399\n",
            "0.02995407\n",
            "0.02994076\n",
            "0.029941235\n",
            "0.029937616\n",
            "0.029944101\n",
            "0.029933073\n",
            "0.02993607\n",
            "0.029931499\n",
            "0.029938411\n",
            "0.029931847\n",
            "0.029926917\n",
            "0.029929671\n",
            "0.029922351\n",
            "0.029954894\n",
            "0.029946009\n",
            "0.029995134\n",
            "0.030143341\n",
            "0.030240433\n",
            "0.029943105\n",
            "0.02999024\n",
            "0.029934525\n",
            "0.029922677\n",
            "0.029929737\n",
            "0.029971574\n",
            "0.03081194\n",
            "0.03003078\n",
            "0.029967656\n",
            "0.029931758\n",
            "0.029921481\n",
            "0.029910166\n",
            "0.029915333\n",
            "0.029910762\n",
            "0.029910201\n",
            "0.029904462\n",
            "0.02990492\n",
            "0.029902643\n",
            "0.029902207\n",
            "0.029900875\n",
            "0.029900253\n",
            "0.029899275\n",
            "0.029898588\n",
            "0.029897792\n",
            "0.02989711\n",
            "0.029896397\n",
            "0.029895732\n",
            "0.029895067\n",
            "0.02989444\n",
            "0.02989382\n",
            "0.02989321\n",
            "0.029892601\n",
            "0.029891988\n",
            "0.029891388\n",
            "0.029890805\n",
            "0.029890236\n",
            "0.029889677\n",
            "0.029889125\n",
            "0.02988858\n",
            "0.029888036\n",
            "0.029887501\n",
            "0.029886978\n",
            "0.029886452\n",
            "0.029885938\n",
            "0.029885428\n",
            "0.029884927\n",
            "0.029884432\n",
            "0.02988394\n",
            "0.029883454\n",
            "0.029882977\n",
            "0.029882502\n",
            "0.029882034\n",
            "0.029881574\n",
            "0.029881116\n",
            "0.029880662\n",
            "0.029880213\n",
            "0.02987977\n",
            "0.029879333\n",
            "0.029878894\n",
            "0.029878464\n",
            "0.029878026\n",
            "0.029877603\n",
            "0.02987717\n",
            "0.029876735\n",
            "0.029876314\n",
            "0.0298759\n",
            "0.029875493\n",
            "0.02987509\n",
            "0.029874694\n",
            "0.029874299\n",
            "0.029873908\n",
            "0.029873526\n",
            "0.029873144\n",
            "0.029872762\n",
            "0.029872386\n",
            "0.029872019\n",
            "0.029871652\n",
            "0.029871287\n",
            "0.029870924\n",
            "0.029870566\n",
            "0.029870208\n",
            "0.02986986\n",
            "0.02986951\n",
            "0.029869165\n",
            "0.02986882\n",
            "0.02986848\n",
            "0.029868145\n",
            "0.029867807\n",
            "0.029867476\n",
            "0.029867146\n",
            "0.029866816\n",
            "0.02986649\n",
            "0.029866163\n",
            "0.029865839\n",
            "0.02986552\n",
            "0.029865202\n",
            "0.029864883\n",
            "0.02986457\n",
            "0.029864257\n",
            "0.029863948\n",
            "0.02986364\n",
            "0.029863333\n",
            "0.02986303\n",
            "0.02986273\n",
            "0.029862426\n",
            "0.029862126\n",
            "0.02986183\n",
            "0.029861536\n",
            "0.029861242\n",
            "0.029860951\n",
            "0.02986066\n",
            "0.02986037\n",
            "0.029860081\n",
            "0.029859794\n",
            "0.029859511\n",
            "0.029859228\n",
            "0.029858947\n",
            "0.029858667\n",
            "0.029858386\n",
            "0.029858112\n",
            "0.029857833\n",
            "0.02985756\n",
            "0.029857283\n",
            "0.029857013\n",
            "0.029856745\n",
            "0.029856477\n",
            "0.029856209\n",
            "0.029855946\n",
            "0.029855682\n",
            "0.02985542\n",
            "0.029855158\n",
            "0.0298549\n",
            "0.029854642\n",
            "0.029854387\n",
            "0.029854136\n",
            "0.02985388\n",
            "0.029853629\n",
            "0.029853381\n",
            "0.029853132\n",
            "0.029852886\n",
            "0.02985264\n",
            "0.029852396\n",
            "0.029852156\n",
            "0.029851913\n",
            "0.029851675\n",
            "0.029851438\n",
            "0.029851202\n",
            "0.029850964\n",
            "0.029850733\n",
            "0.0298505\n",
            "0.029850267\n",
            "0.029850036\n",
            "0.029849807\n",
            "0.02984958\n",
            "0.02984935\n",
            "0.029849125\n",
            "0.029848902\n",
            "0.02984868\n",
            "0.029848456\n",
            "0.029848235\n",
            "0.02984801\n",
            "0.029847793\n",
            "0.029847573\n",
            "0.029847357\n",
            "0.02984714\n",
            "0.029846923\n",
            "0.029846711\n",
            "0.029846495\n",
            "0.029846286\n",
            "0.029846072\n",
            "0.029845862\n",
            "0.02984565\n",
            "0.029845444\n",
            "0.029845232\n",
            "0.02984503\n",
            "0.029844822\n",
            "0.029844617\n",
            "0.029844414\n",
            "0.029844211\n",
            "0.029844007\n",
            "0.02984381\n",
            "0.029843606\n",
            "0.029843409\n",
            "0.029843211\n",
            "0.029843012\n",
            "0.029842816\n",
            "0.029842624\n",
            "0.029842427\n",
            "0.029842233\n",
            "0.029842041\n",
            "0.02984185\n",
            "0.029841658\n",
            "0.029841468\n",
            "0.02984128\n",
            "0.029841091\n",
            "0.029840905\n",
            "0.029840717\n",
            "0.02984053\n",
            "0.029840343\n",
            "0.029840158\n",
            "0.029839976\n",
            "0.029839791\n",
            "0.029839613\n",
            "0.029839426\n",
            "0.029839246\n",
            "0.029839065\n",
            "0.029838886\n",
            "0.029838704\n",
            "0.029838525\n",
            "0.029838352\n",
            "0.02983817\n",
            "0.029837992\n",
            "0.029837817\n",
            "0.029837642\n",
            "0.029837467\n",
            "0.02983729\n",
            "0.029837117\n",
            "0.029836943\n",
            "0.02983677\n",
            "0.029836599\n",
            "0.029836427\n",
            "0.029836256\n",
            "0.029836087\n",
            "0.029835915\n",
            "0.029835748\n",
            "0.029835578\n",
            "0.029835412\n",
            "0.029835243\n",
            "0.029835075\n",
            "0.02983491\n",
            "0.029834744\n",
            "0.029834578\n",
            "0.029834416\n",
            "0.029834246\n",
            "0.029834084\n",
            "0.029833922\n",
            "0.029833755\n",
            "0.029833592\n",
            "0.029833432\n",
            "0.02983327\n",
            "0.029833106\n",
            "0.029832946\n",
            "0.029832784\n",
            "0.029832624\n",
            "0.029832466\n",
            "0.029832305\n",
            "0.029832143\n",
            "0.029831985\n",
            "0.02983183\n",
            "0.029831668\n",
            "0.029831512\n",
            "0.029831354\n",
            "0.029831197\n",
            "0.029831039\n",
            "0.029830884\n",
            "0.02983073\n",
            "0.029830571\n",
            "0.029830417\n",
            "0.029830264\n",
            "0.02983011\n",
            "0.029829955\n",
            "0.029829802\n",
            "0.02982965\n",
            "0.029829495\n",
            "0.029829344\n",
            "0.02982919\n",
            "0.029829038\n",
            "0.029828884\n",
            "0.029828733\n",
            "0.029828584\n",
            "0.029828433\n",
            "0.02982828\n",
            "0.029828131\n",
            "0.029827982\n",
            "0.02982783\n",
            "0.029827684\n",
            "0.029827535\n",
            "0.029827386\n",
            "0.029827239\n",
            "0.029827094\n",
            "0.029826948\n",
            "0.029826798\n",
            "0.02982665\n",
            "0.029826507\n",
            "0.029826362\n",
            "0.029826216\n",
            "0.029826071\n",
            "0.029825928\n",
            "0.029825784\n",
            "0.02982564\n",
            "0.0298255\n",
            "0.029825358\n",
            "0.029825214\n",
            "0.029825075\n",
            "0.029824933\n",
            "0.029824793\n",
            "0.029824654\n",
            "0.029824514\n",
            "0.029824376\n",
            "0.02982424\n",
            "0.0298241\n",
            "0.029823964\n",
            "0.029823827\n",
            "0.029823694\n",
            "0.029823558\n",
            "0.029823426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2zM67IW-wWP"
      },
      "source": [
        "# Generating Phonemes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZK1B4U42Ki1",
        "outputId": "19924a10-7c98-41ca-dd6a-c0fe93941749"
      },
      "source": [
        "lstm = LSTMSeq2SeqAttn(input_vocab_size=config['input_vocab_size'],\n",
        "                       target_vocab_size=config['output_vocab_size'],\n",
        "                        )\n",
        "\n",
        "lstm.init_weights_and_state((shapes.signature(input), shapes.signature(labels)))\n",
        "\n",
        "lstm.weights = params\n",
        "\n",
        "graphemes = [['a', 'b', 'a', 'c', 'k']]\n",
        "phonemes = [[]]\n",
        "\n",
        "grapheme_token_ids = data_loader.encode_batch(graphemes)\n",
        "phoneme_token_ids = data_loader.encode_batch(phonemes, key='phonemes', add_eos=False)\n",
        "\n",
        "for i in range(config['max_length']['phonemes']-1):\n",
        "    \n",
        "    logits = lstm((grapheme_token_ids, \n",
        "                              phoneme_token_ids))[0]\n",
        "    \n",
        "    preds = jnp.argmax(logits[:,i,:], axis=-1)\n",
        "    \n",
        "    phoneme_token_ids = jax.ops.index_update(phoneme_token_ids, \n",
        "                                             jax.ops.index[:,i+1],\n",
        "                                             preds)\n",
        "    \n",
        "print(phoneme_token_ids)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1 66 40 20 26  2  2  2  2  2  2  2  2  2  2  2  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQLnIXf3qiHa",
        "outputId": "85977cb9-64ef-43ae-cbd0-5e9183baa0b4"
      },
      "source": [
        "print(data_loader.decode_batch(phoneme_token_ids))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['ə', 'ˈb', 'æ', 'k']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}